{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from degree_days import dds\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from common import compute_rmse_fraction, contri, get_tensor, create_region_df_dfc_static\n",
    "from create_matrix import *\n",
    "from tensor_custom_core import *\n",
    "\n",
    "appliance_index = {appliance: APPLIANCES_ORDER.index(appliance) for appliance in APPLIANCES_ORDER}\n",
    "\n",
    "APPLIANCES = ['fridge', 'hvac', 'wm', 'mw', 'oven', 'dw']\n",
    "region = \"Austin\"\n",
    "year = 2014\n",
    "\n",
    "import os\n",
    "\n",
    "def un_normalize(x, maximum, minimum):\n",
    "    return (maximum - minimum) * x + minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "case=2\n",
    "a = 3\n",
    "b = 2\n",
    "source = 'Austin'\n",
    "target = 'SanDiego'\n",
    "constant_use = 'True'\n",
    "start = 1\n",
    "stop = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_df, source_dfc, source_tensor, source_static = create_region_df_dfc_static(source, year, start, stop)\n",
    "target_df, target_dfc, target_tensor, target_static = create_region_df_dfc_static(target, year, start, stop)\n",
    "\n",
    "# # using cosine similarity to compute L\n",
    "source_L = get_L(source_static)\n",
    "target_L = get_L(target_static)\n",
    "\n",
    "# Seasonal constant constraints\n",
    "if constant_use == 'True':\n",
    "    T_constant = np.ones(stop-start).reshape(-1 , 1)\n",
    "else:\n",
    "    T_constant = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_copy = source_tensor.copy()\n",
    "tensor_copy[:, 1:, :] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from degree_days import dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For HVAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensor_appliance(df, dfc, appliance):\n",
    "    start, stop = 1, 13\n",
    "    energy_cols = np.array(\n",
    "        [['%s_%d' % (appliance, month) for month in range(start, stop)] ]).flatten()\n",
    "    static_cols = ['area', 'total_occupants', 'num_rooms']\n",
    "    static_df = df[static_cols]\n",
    "    static_df = static_df.div(static_df.max())\n",
    "    weather_values = np.array(dds[2014][region][start - 1:stop - 1]).reshape(-1, 1)\n",
    "\n",
    "    dfc = df.copy()\n",
    "\n",
    "    df = dfc[energy_cols]\n",
    "    col_max = df.max().max()\n",
    "    col_min = df.min().min()\n",
    "    # df = (1.0 * (df - col_min)) / (col_max - col_min)\n",
    "    tensor = df.values.reshape((len(df), 1, stop - start))\n",
    "    M, N, O = tensor.shape\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvac_tensor = get_tensor_appliance(source_df, source_dfc, 'fridge')\n",
    "agg_tensor = get_tensor_appliance(source_df, source_dfc, 'aggregate')\n",
    "tensor = np.concatenate((agg_tensor, hvac_tensor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known_homes = []\n",
    "random_list =  random.sample(range(num_train), 50)\n",
    "random_list =[x+1 for x in random_list]\n",
    "\n",
    "pos = random_list[:50]\n",
    "known_homes = known_homes+pos\n",
    "# aggregate readings for all homes are known\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse_fraction(appliance, pred_df, region='Austin', year=2014):\n",
    "\tappliance_df = create_matrix_region_appliance_year(region, year, appliance)\n",
    "\n",
    "\tif appliance == \"hvac\":\n",
    "\t\tstart, stop = 5, 11\n",
    "\telse:\n",
    "\t\tstart, stop = 1, 13\n",
    "\tpred_df = pred_df.copy()\n",
    "\tpred_df.columns = [['%s_%d' % (appliance, month) for month in range(start, stop)]]\n",
    "\tgt_df = appliance_df[pred_df.columns].ix[pred_df.index]\n",
    "\n",
    "\taggregate_df = appliance_df.ix[pred_df.index][['aggregate_%d' % month for month in range(start, stop)]]\n",
    "\n",
    "\taggregate_df.columns = gt_df.columns\n",
    "\trows, cols = np.where((aggregate_df < 100))\n",
    "\tfor r, c in zip(rows, cols):\n",
    "\t\tr_i, c_i = aggregate_df.index[r], aggregate_df.columns[c]\n",
    "\t\taggregate_df.loc[r_i, c_i] = np.NaN\n",
    "\n",
    "\tgt_fraction = gt_df.div(aggregate_df) * 100\n",
    "\tpred_fraction = pred_df.div(aggregate_df) * 100\n",
    "\n",
    "\t# Capping it to 100%\n",
    "\tpred_fraction[pred_fraction > 100] = 100.\n",
    "\n",
    "\tgt_fraction_dropna = gt_fraction.unstack().dropna()\n",
    "\tpred_fraction_dropna = pred_fraction.unstack().dropna()\n",
    "\tindex_intersection = gt_fraction_dropna.index.intersection(pred_fraction_dropna.index)\n",
    "\tgt_fraction_dropna = gt_fraction_dropna.ix[index_intersection]\n",
    "\tpred_fraction_dropna = pred_fraction_dropna.ix[index_intersection]\n",
    "\tdifference_error = (gt_fraction_dropna - pred_fraction_dropna).abs()\n",
    "\n",
    "\trms = np.sqrt(mean_squared_error(gt_fraction_dropna, pred_fraction_dropna))\n",
    "\treturn gt_fraction_dropna, pred_fraction_dropna, rms, difference_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ix:  Int64Index([ 22,  26,  48,  59,  68,  77,  86,  93,  94, 101, 114, 115, 121,\n",
      "            130, 135, 160, 171, 187, 222, 243, 252, 267, 297, 347, 364, 370,\n",
      "            410, 434, 436, 457, 470, 484, 491, 499, 503, 507, 508, 545, 555,\n",
      "            580, 585, 624, 645, 661, 668, 739, 744, 772, 781, 861, 871, 878,\n",
      "            890, 898],\n",
      "           dtype='int64')\n",
      "670.983049446\n",
      "111.941447556\n",
      "40.8817683068\n",
      "test_ix:  Int64Index([ 936,  946,  954,  974,  980,  994, 1037, 1069, 1086, 1103, 1105,\n",
      "            1153, 1169, 1185, 1192, 1202, 1283, 1310, 1314, 1331, 1334, 1403,\n",
      "            1415, 1463, 1464, 1479, 1500, 1507, 1508, 1551, 1577, 1586, 1589,\n",
      "            1601, 1617, 1632, 1642, 1681, 1697, 1700, 1714, 1718, 1782, 1790,\n",
      "            1791, 1792, 1796, 1800, 1801, 1854, 1889, 1947, 1953, 1994],\n",
      "           dtype='int64')\n",
      "653.576385008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-1910f75d1df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                                               \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                                               \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                                                               T_known=T_constant)\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# Generate tensor_copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xox/git/scalable-nilm/aaai18/code/tensor_custom_core.py\u001b[0m in \u001b[0;36mlearn_HAT_adagrad_graph\u001b[0;34m(case, tensor, L, num_home_factors, num_season_factors, num_iter, lr, dis, lam, random_seed, eps, A_known, T_known)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;31m# GD procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mdel_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msum_square_gradients_A\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xox/Library/Python/2.7/lib/python/site-packages/autograd/convenience_wrappers.pyc\u001b[0m in \u001b[0;36mgradfun_rearranged\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradfun_rearranged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmulti_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgradfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradfun_rearranged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xox/Library/Python/2.7/lib/python/site-packages/autograd/errors.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_extra_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import nnls\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import random\n",
    "\n",
    "random_list =  random.sample(range(533), 533)\n",
    "random_list =[x+1 for x in random_list]\n",
    "\n",
    "n_splits = 10\n",
    "case = 2\n",
    "a = 3\n",
    "b = 3\n",
    "cost = 'abs'\n",
    "app = 'fridge'\n",
    "known_homes = []\n",
    "\n",
    "   \n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "for random_seed in range(1):\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    for train_max, test in kf.split(source_df):\n",
    "            \n",
    "            num_train = len(train_max)\n",
    "            num_test = len(test)\n",
    "\n",
    "            train = train_max\n",
    "           \n",
    "            # get the index of training and testing data\n",
    "            train_ix = source_df.index[train]\n",
    "            test_ix = source_df.index[test]\n",
    "            print \"test_ix: \", test_ix\n",
    "\n",
    "            # create the tensor\n",
    "            train_test_ix = np.concatenate([train_ix, test_ix])\n",
    "            df_t, dfc_t = source_df.ix[train_test_ix], source_dfc.ix[train_test_ix]\n",
    "            app_tensor = get_tensor_appliance(df_t, dfc_t, app)\n",
    "            agg_tensor = get_tensor_appliance(df_t, dfc_t, 'aggregate')\n",
    "            \n",
    "            tensor = np.concatenate((agg_tensor, app_tensor), axis=1)\n",
    "            \n",
    "            \n",
    "            random_list =  random.sample(range(num_train), 50)\n",
    "            random_list =[x+1 for x in random_list]\n",
    "\n",
    "            for month in range(1):\n",
    "                if month == 0:\n",
    "                    tensor_copy = tensor.copy()\n",
    "                    tensor_copy[:,:,:] = np.NaN\n",
    "                    pos = random_list[:50]\n",
    "                    known_homes = known_homes + pos\n",
    "                    # aggregate readings for all homes are known\n",
    "                    tensor_copy[:, 0, :] = tensor[:, 0, :]\n",
    "                    # appliance readings for selected homes are known\n",
    "                    tensor_copy[known_homes, 1, :] = tensor[known_homes, 1, :]\n",
    "                    tensor_copy = tensor_copy[:, :, 0:(month+2)]\n",
    "                    \n",
    "                    T_constant = np.ones(month+2).reshape(-1,1)\n",
    "                    H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor_copy,\n",
    "                                                                                source_L,\n",
    "                                                                              a,\n",
    "                                                                              b,\n",
    "                                                                              num_iter=1300,\n",
    "                                                                              lr=0.1, dis=True,\n",
    "                                                                              lam=0,\n",
    "                                                                              T_known=T_constant)\n",
    "                else:\n",
    "                    # Generate tensor_copy\n",
    "                    tensor_copy = tensor.copy()\n",
    "                    tensor_copy[:,:,:] = np.NaN\n",
    "                    known_homes = known_homes + pos\n",
    "                    # aggregate readings for all homes are known\n",
    "                    tensor_copy[:, 0, :] = tensor[:, 0, :]\n",
    "                    # appliance readings for selected homes are known\n",
    "                    tensor_copy[known_homes, 1, :] = tensor[known_homes, 1, :]\n",
    "                    tensor_copy = tensor_copy[:, :, 0:(month+2)]\n",
    "                    \n",
    "                    ####\n",
    "                    T_constant = np.ones(month+2).reshape(-1,1)\n",
    "                    H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor_copy,\n",
    "                                                                                source_L,\n",
    "                                                                              a,\n",
    "                                                                              b,\n",
    "                                                                              num_iter=1300,\n",
    "                                                                              lr=0.1, dis=True,\n",
    "                                                                              lam=0,\n",
    "                                                                              T_known=T_constant)\n",
    "                \n",
    "                ## Get the home ids\n",
    "                ### Based on the Home factors\n",
    "                HAT = multiply_case(H, A, T, case)\n",
    "                \n",
    "                pred_test = pd.DataFrame(HAT[num_train:, 1, :], index=test_ix)\n",
    "                rms_test, error_test = compute_rmse_fraction(app, pred_test, 'Austin', 1, month+3)[2:]\n",
    "                pred_train = pd.DataFrame(HAT[:num_train, 1, :], index=train_ix)\n",
    "                rms_train, error_train = compute_rmse_fraction(app, pred_train, 'Austin', 1, month+3)[2:]\n",
    "                \n",
    "                \n",
    "                H_train = H[:num_train]\n",
    "                H_train = H_train/np.max(H_train)\n",
    "                label = KMeans(n_clusters=10, random_state=0).fit_predict(H_train)\n",
    "                \n",
    "                error_home = pd.concat([error_train[appliance + \"_{}\".format(start)], \n",
    "                   error_train[appliance + \"_{}\".format(start+1)]],axis=1)\n",
    "                for i in range(start+2, end):\n",
    "                    error_home = pd.concat([error_home, error_train[appliance + \"_{}\".format(i)]], axis = 1)\n",
    "                app = np.sqrt((error_home**2).mean(axis=1))\n",
    "                \n",
    "                max_error = 0\n",
    "                max_cluster = -1\n",
    "                for i in range(10):\n",
    "                    index_set = find(label, i)\n",
    "                    ids = train_ix[k]\n",
    "                    mean = cluster_mean_error(app, ids)\n",
    "                    if mean > max_error:\n",
    "                        max_error = mean\n",
    "                        max_cluster = i\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find(lst, value):\n",
    "    return [i for i, x in enumerate(lst) if x==value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8433883752\n",
      "[0, 2, 9, 10, 20, 25, 64, 71, 106, 110, 125, 184, 193, 219, 227, 238, 240, 256, 262, 287, 292, 294, 321, 341, 342, 345, 349, 357, 370, 379, 392, 409, 411, 412, 414, 421, 422, 446, 448, 450, 463, 466, 478]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "H_train = H[:num_train]\n",
    "print np.max(H_train)\n",
    "H_train = H_train/np.max(H_train)\n",
    "label = KMeans(n_clusters=10, random_state=0).fit_predict(H_train)\n",
    "k = find(label,5)\n",
    "print k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_mean_error(app_error, index):\n",
    "    sum_error = 0\n",
    "    for i in index:\n",
    "#         print app_error[i]\n",
    "        if i in app_error.index:\n",
    "            sum_error += app_error[i]\n",
    "    return sum_error/len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.63938537858\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "end = 3\n",
    "appliance = 'fridge'\n",
    "error_home = pd.concat([error_train[appliance + \"_{}\".format(start)], \n",
    "                   error_train[appliance + \"_{}\".format(start+1)]],axis=1)\n",
    "\n",
    "for i in range(start+2, end):\n",
    "    error_home = pd.concat([error_home, error_train[appliance + \"_{}\".format(i)]], axis = 1)\n",
    "app = np.sqrt((error_home**2).mean(axis=1))\n",
    "\n",
    "ids = train_ix[k]\n",
    "mean = cluster_mean_error(app, ids)\n",
    "print mean\n",
    "\n",
    "# app.index\n",
    "# app[936]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(HAT, index, domain):\n",
    "    pred = {}\n",
    "    for appliance_name, appliance_id in appliance_index.iteritems():\n",
    "        pred[appliance_name] = []\n",
    "        pred[appliance_name].append(pd.DataFrame(HAT[:,appliance_id,:], index=index))\n",
    "#     print pred\n",
    "    for appliance_name, appliance_id in appliance_index.iteritems():\n",
    "        pred[appliance_name] = pd.concat(pred[appliance_name]).ix[index]\n",
    "    \n",
    "    \n",
    "#     pred = pd.DataFrame(pred)\n",
    "    err_all = {}\n",
    "    err = {}\n",
    "    for appliance in APPLIANCES_ORDER[1:]:        \n",
    "        if appliance==\"hvac\":\n",
    "            err_all[appliance], err[appliance] = compute_rmse_fraction(appliance, pred[appliance][range(4, 10)], domain)[2:]\n",
    "        else:   \n",
    "            err_all[appliance], err[appliance] = compute_rmse_fraction(appliance, pred[appliance], domain)[2:]\n",
    "\n",
    "    err_app = {}\n",
    "    for appliance in APPLIANCES_ORDER[1:]:\n",
    "        if appliance == 'hvac':\n",
    "            start, end = 5, 11\n",
    "        else:\n",
    "            start, end = 1, 13\n",
    "\n",
    "        error_home = pd.concat([err[appliance][appliance + \"_{}\".format(start)], \n",
    "                           err[appliance][appliance + \"_{}\".format(start+1)]],axis=1)\n",
    "\n",
    "        for i in range(start+2, end):\n",
    "            error_home = pd.concat([error_home, err[appliance][appliance + \"_{}\".format(i)]], axis = 1)\n",
    "        app = np.sqrt((error_home**2).mean(axis=1))\n",
    "        err_app[appliance] = app\n",
    "\n",
    "    result = (pd.DataFrame(err_app).fillna(0)*pd.Series(contri[domain])).sum(axis=1)\n",
    "        \n",
    "        \n",
    "    return err_all, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927.359618339\n",
      "327.132788609\n",
      "174.115738163\n"
     ]
    }
   ],
   "source": [
    "H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor_copy,\n",
    "                                                            source_L,\n",
    "                                                          a,\n",
    "                                                          b,\n",
    "                                                          num_iter=1300,\n",
    "                                                          lr=0.1, dis=True,\n",
    "                                                          lam=0,\n",
    "                                                          T_known=T_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.66345507,  1.8757803 ,  1.69049574],\n",
       "       [ 5.56361928,  5.50529646,  5.50663552],\n",
       "       [ 1.3197905 ,  1.78687358,  1.804727  ],\n",
       "       ..., \n",
       "       [ 2.73141718,  2.82475159,  2.26333804],\n",
       "       [ 2.24214654,  2.14861078,  2.76485631],\n",
       "       [ 2.99715369,  3.21232919,  3.28624526]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yj9xs/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvac\n",
      "fridge\n",
      "mw\n",
      "dw\n",
      "wm\n",
      "oven\n"
     ]
    }
   ],
   "source": [
    "err_all, err_home = compute_error(HATs[-1], source_df.index, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.594146728515625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.ix[max_index]['aggregate_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ((pd.Series(err_all) * pd.Series(contri['Austin']))).sum()# error = (pd.DataFrame(out)*pd.Series(contri['SanDiego'])).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_index = err_home.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "position = source_df.index.get_loc(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_copy[position] = source_tensor[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(5)\n",
    "random_list =  random.sample(range(533), 533)\n",
    "random_list =[x+1 for x in random_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yj9xs/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1021914905\n",
      "61\n",
      "0 22.1222043053\n",
      "47\n",
      "1 19.8106416228\n",
      "476\n",
      "2 18.4501308997\n",
      "178\n",
      "3 16.5123081787\n",
      "518\n",
      "4 17.1332797771\n",
      "494\n",
      "5 17.0051122383\n",
      "45\n",
      "6 16.8313282303\n",
      "318\n",
      "7 14.7923720999\n",
      "131\n",
      "8 14.7421349938\n",
      "346\n",
      "9 14.6333198034\n",
      "153\n",
      "10 13.6021757488\n",
      "59\n",
      "11 13.596117084\n",
      "439\n",
      "12 13.4803772491\n",
      "425\n",
      "13 13.5001449042\n",
      "494\n",
      "14 13.5001449042\n",
      "494\n",
      "15 13.5001449042\n",
      "494\n",
      "16 13.5001449042\n",
      "494\n",
      "17 13.5001449042\n",
      "494\n",
      "18 13.5001449042\n",
      "494\n",
      "19 13.5001449042\n"
     ]
    }
   ],
   "source": [
    "prediction_error = []\n",
    "\n",
    "tensor_copy = source_tensor.copy()\n",
    "tensor_copy[:, 1:, :] = np.NaN\n",
    "H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor_copy,\n",
    "                                                            source_L,\n",
    "                                                          a,\n",
    "                                                          b,\n",
    "                                                          num_iter=1300,\n",
    "                                                          lr=0.1, dis=False,\n",
    "                                                          lam=0,\n",
    "                                                      T_known=T_constant)\n",
    "err_all, err_home = compute_error(HATs[-1], source_df.index, source)\n",
    "overall_error = ((pd.Series(err_all) * pd.Series(contri['Austin']))).sum()\n",
    "print overall_error\n",
    "\n",
    "for i in range(20):\n",
    "    max_index = err_home.argmax()\n",
    "    position = source_df.index.get_loc(max_index)\n",
    "#     position = random_list[i]\n",
    "    print position\n",
    "    tensor_copy[position] = source_tensor[position]\n",
    "    \n",
    "    H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor_copy,\n",
    "                                                            source_L,\n",
    "                                                          a,\n",
    "                                                          b,\n",
    "                                                          num_iter=1300,\n",
    "                                                          lr=0.1, dis=False,\n",
    "                                                          lam=0,\n",
    "                                                          T_known=T_constant)\n",
    "    err_all, err_home = compute_error(HATs[-1], source_df.index, source)\n",
    "    overall_error = ((pd.Series(err_all) * pd.Series(contri['Austin']))).sum()\n",
    "    print i, overall_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
