{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from common import compute_rmse_fraction, contri, get_tensor, create_region_df_dfc_static\n",
    "from create_matrix import *\n",
    "from tensor_custom_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_df, target_dfc, target_tensor, target_static = create_region_df_dfc_static('Austin', 2014)\n",
    "target_L = get_L(target_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case = 2\n",
    "tensor = target_tensor\n",
    "num_home = 3\n",
    "num_season = 3\n",
    "iters = 1300\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 228 ms, total: 36.5 s\n",
      "Wall time: 9.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "H, A, T, Hs, As, Ts, HATs, costs = learn_HAT_adagrad_graph(case, tensor, target_L, num_home, num_season, num_iter = iters, lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M, N,O = tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['M'], params['N'], params['O'] = tensor.shape\n",
    "params['a'] = num_home\n",
    "params['b'] = num_season\n",
    "H_dim_chars = list(cases[case]['HA'].split(\",\")[0].strip())\n",
    "H_dim = tuple(params[x] for x in H_dim_chars)\n",
    "A_dim_chars = list(cases[case]['HA'].split(\",\")[1].split(\"-\")[0].strip())\n",
    "A_dim = tuple(params[x] for x in A_dim_chars)\n",
    "S_dim_chars = list(cases[case]['HAT'].split(\",\")[1].split(\"-\")[0].strip())\n",
    "S_dim = tuple(params[x] for x in S_dim_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = ~np.isnan(tensor)\n",
    "ones = np.ones(mask.shape)\n",
    "mask = (mask*ones).astype(int)\n",
    "mask = Variable(torch.ByteTensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yj9xs/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py:699: UserWarning: masked_copy is deprecated and renamed to masked_scatter, and will be removed in v0.3\n",
      "  warnings.warn(\"masked_copy is deprecated and renamed to masked_scatter, and will be removed in v0.3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 237.4827117919922)\n",
      "(10, 234.4619598388672)\n",
      "(20, 217.34559631347656)\n",
      "(30, 177.96591186523438)\n",
      "(40, 123.04818725585938)\n",
      "(50, 97.9781494140625)\n",
      "(60, 90.14020538330078)\n",
      "(70, 86.66566467285156)\n",
      "(80, 83.679931640625)\n",
      "(90, 80.75070190429688)\n",
      "(100, 78.82691192626953)\n",
      "(110, 77.34779357910156)\n",
      "(120, 76.09830474853516)\n",
      "(130, 74.8541030883789)\n",
      "(140, 73.33822631835938)\n",
      "(150, 71.34660339355469)\n",
      "(160, 68.76170349121094)\n",
      "(170, 66.07138061523438)\n",
      "(180, 64.0447998046875)\n",
      "(190, 62.74252700805664)\n",
      "(200, 62.012855529785156)\n",
      "(210, 61.569881439208984)\n",
      "(220, 61.285491943359375)\n",
      "(230, 61.10401153564453)\n",
      "(240, 60.98958206176758)\n",
      "(250, 60.9160041809082)\n",
      "(260, 60.86655044555664)\n",
      "(270, 60.83244705200195)\n",
      "(280, 60.80561447143555)\n",
      "(290, 60.78407669067383)\n",
      "(300, 60.7645263671875)\n",
      "(310, 60.74620056152344)\n",
      "(320, 60.72829818725586)\n",
      "(330, 60.71091842651367)\n",
      "(340, 60.69142150878906)\n",
      "(350, 60.67185974121094)\n",
      "(360, 60.6512451171875)\n",
      "(370, 60.62892150878906)\n",
      "(380, 60.60439682006836)\n",
      "(390, 60.5779914855957)\n",
      "(400, 60.548744201660156)\n",
      "(410, 60.51687240600586)\n",
      "(420, 60.48198318481445)\n",
      "(430, 60.44114303588867)\n",
      "(440, 60.395809173583984)\n",
      "(450, 60.345184326171875)\n",
      "(460, 60.28800964355469)\n",
      "(470, 60.224002838134766)\n",
      "(480, 60.1502571105957)\n",
      "(490, 60.06782150268555)\n",
      "(500, 59.97578430175781)\n",
      "(510, 59.873985290527344)\n",
      "(520, 59.76353073120117)\n",
      "(530, 59.645931243896484)\n",
      "(540, 59.52267837524414)\n",
      "(550, 59.39829635620117)\n",
      "(560, 59.27576446533203)\n",
      "(570, 59.15886688232422)\n",
      "(580, 59.05152893066406)\n",
      "(590, 58.954254150390625)\n",
      "(600, 58.87017059326172)\n",
      "(610, 58.79956817626953)\n",
      "(620, 58.741878509521484)\n",
      "(630, 58.69587707519531)\n",
      "(640, 58.66009521484375)\n",
      "(650, 58.63288879394531)\n",
      "(660, 58.61255645751953)\n",
      "(670, 58.59767150878906)\n",
      "(680, 58.58694839477539)\n",
      "(690, 58.581233978271484)\n",
      "(700, 58.57416915893555)\n",
      "(710, 58.57047653198242)\n",
      "(720, 58.5692138671875)\n",
      "(730, 58.568058013916016)\n",
      "(740, 58.566322326660156)\n",
      "(750, 58.56454086303711)\n",
      "(760, 58.563663482666016)\n",
      "(770, 58.563228607177734)\n",
      "(780, 58.562984466552734)\n",
      "(790, 58.562828063964844)\n",
      "(800, 58.5627326965332)\n",
      "(810, 58.56266784667969)\n",
      "(820, 58.56264877319336)\n",
      "(830, 58.56261444091797)\n",
      "(840, 58.562583923339844)\n",
      "(850, 58.56257247924805)\n",
      "(860, 58.562564849853516)\n",
      "(870, 58.56256103515625)\n",
      "(880, 58.562564849853516)\n",
      "(890, 58.56256103515625)\n",
      "(900, 58.56271743774414)\n",
      "(910, 58.56489944458008)\n",
      "(920, 58.562931060791016)\n",
      "(930, 58.56513977050781)\n",
      "(940, 58.56437683105469)\n",
      "(950, 58.56279754638672)\n",
      "(960, 58.56263732910156)\n",
      "(970, 58.562618255615234)\n",
      "(980, 58.562591552734375)\n",
      "(990, 58.56257247924805)\n",
      "(1000, 58.56256103515625)\n",
      "(1010, 58.562557220458984)\n",
      "(1020, 58.56255340576172)\n",
      "(1030, 58.56255340576172)\n",
      "(1040, 58.56255340576172)\n",
      "(1050, 58.56255340576172)\n",
      "(1060, 58.56257629394531)\n",
      "(1070, 58.56255340576172)\n",
      "(1080, 58.562557220458984)\n",
      "(1090, 58.562557220458984)\n",
      "(1100, 58.562652587890625)\n",
      "(1110, 58.56298065185547)\n",
      "(1120, 58.56300354003906)\n",
      "(1130, 58.562705993652344)\n",
      "(1140, 58.562591552734375)\n",
      "(1150, 58.56263732910156)\n",
      "(1160, 58.563255310058594)\n",
      "(1170, 58.56422424316406)\n",
      "(1180, 58.5638313293457)\n",
      "(1190, 58.56372833251953)\n",
      "(1200, 58.56296157836914)\n",
      "(1210, 58.562686920166016)\n",
      "(1220, 58.562583923339844)\n",
      "(1230, 58.562557220458984)\n",
      "(1240, 58.56255340576172)\n",
      "(1250, 58.56256866455078)\n",
      "(1260, 58.56258773803711)\n",
      "(1270, 58.56369400024414)\n",
      "(1280, 58.56257629394531)\n",
      "(1290, 58.56264114379883)\n",
      "CPU times: user 16.3 s, sys: 84 ms, total: 16.4 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "H = Variable(torch.randn(*H_dim), requires_grad=True)\n",
    "A = Variable(torch.randn(*A_dim), requires_grad=True)\n",
    "S = Variable(torch.randn(*S_dim), requires_grad=True)\n",
    "tensor_copy = Variable(torch.Tensor(tensor), requires_grad=False)\n",
    "tensor_copy[tensor_copy != tensor_copy] = 0\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam([H, A, S], lr=learning_rate)\n",
    "\n",
    "for t in range(1300):\n",
    "    \n",
    "    HA = torch.mm(H, A.view(params['a'], params['N']*params['b'])).view(params['M'], params['N'], params['b'])\n",
    "    HAS = torch.mm(HA.view(params['M']*params['N'], params['b']), S.transpose(0, 1)).view(params['M'], params['N'], params['O'])\n",
    "        \n",
    "    loss_all = (HAS - tensor_copy).pow(2)\n",
    "    loss_masked = Variable(torch.Tensor(M, N, O).zero_()).masked_copy(mask, loss_all)\n",
    "    loss = torch.sqrt(loss_masked.mean())\n",
    "        \n",
    "    if t % 10 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
